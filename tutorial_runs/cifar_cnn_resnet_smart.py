"""
CIFAR-10 æ™ºèƒ½è¯Šæ–­è®­ç»ƒè„šæœ¬ï¼ˆé˜¶æ®µ1ï¼šè¯Šæ–­+å»ºè®®ï¼Œæ•™å­¦å‹å¥½ç‰ˆæœ¬ï¼‰
åŸºäºæ•™ç¨‹ä¸­çš„è®­ç»ƒé—®é¢˜è¯Šæ–­è¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹è®­ç»ƒé—®é¢˜å¹¶æä¾›æ”¹è¿›å»ºè®®
"""
import argparse
import csv
import json
import os
import random
import time
from collections import Counter
from dataclasses import asdict, dataclass
from typing import Dict, List, Literal, Optional, Tuple

import torch
from torch import nn
from torch.utils.data import DataLoader, Subset

try:
    import torchvision
    from torchvision import transforms
except ImportError as exc:
    raise SystemExit("Missing torchvision. Install with: python -m pip install torchvision") from exc


# ============================================================================
# æ¨¡å‹å®šä¹‰
# ============================================================================

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        return self.net(x)


# ============================================================================
# æ•°æ®ç±»å®šä¹‰
# ============================================================================

@dataclass
class EpochRow:
    epoch: int
    train_loss: float
    train_acc: float
    val_loss: float
    val_acc: float


@dataclass
class RunMetrics:
    model: str
    num_params: int
    device: str
    num_epochs: int
    train_subset: int
    test_subset: int
    batch_size: int
    lr: float
    weight_decay: float
    data_augment: bool
    elapsed_sec: float
    final_train_acc: float
    final_val_acc: float
    final_train_loss: float
    final_val_loss: float
    diagnosed_problems: List[str]
    recommendations: List[str]


# ============================================================================
# æ•°æ®è´¨é‡æ£€æŸ¥å™¨ï¼ˆè®­ç»ƒå‰ä¸»åŠ¨ç­–ç•¥ï¼‰
# ============================================================================

class DataQualityChecker:
    """è®­ç»ƒå‰æ•°æ®è´¨é‡æ£€æŸ¥ï¼ˆå¯¹åº”æ•™ç¨‹æ•°æ®æ“ä½œè¡¨-è®­ç»ƒå‰éƒ¨åˆ†ï¼‰"""
    
    @staticmethod
    def check_data_quality(dataset, num_classes=10):
        """EDAå¼æ•°æ®æ£€æŸ¥"""
        issues = []
        recommendations = []
        
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®è´¨é‡é¢„æ£€æŸ¥ï¼ˆè®­ç»ƒå‰ä¸»åŠ¨ç­–ç•¥ï¼‰")
        print("="*60)
        
        # æ£€æŸ¥1: æ•°æ®é‡æ˜¯å¦å……è¶³
        n_samples = len(dataset)
        print(f"âœ“ æ•°æ®é›†å¤§å°: {n_samples} æ ·æœ¬")
        
        if n_samples < 5000:
            issues.append(f"æ•°æ®é‡åå° ({n_samples} < 5000)")
            recommendations.append(
                "å»ºè®®1: ä½¿ç”¨æ•°æ®å¢å¼º (RandomHorizontalFlip + RandomCrop)"
            )
            print(f"  âš ï¸  æ•°æ®é‡åå°: {n_samples} < 5000")
        else:
            print(f"  âœ“ æ•°æ®é‡å……è¶³")
        
        # æ£€æŸ¥2: ç±»åˆ«æ˜¯å¦å¹³è¡¡
        labels = []
        for i in range(min(1000, n_samples)):
            _, label = dataset[i]
            if isinstance(label, torch.Tensor):
                label = label.item()
            labels.append(label)
        
        label_dist = Counter(labels)
        if len(label_dist) > 0:
            max_count = max(label_dist.values())
            min_count = min(label_dist.values())
            imbalance_ratio = max_count / max(min_count, 1)
            
            print(f"âœ“ ç±»åˆ«åˆ†å¸ƒï¼ˆé‡‡æ ·{len(labels)}ä¸ªï¼‰: {dict(label_dist)}")
            
            if imbalance_ratio > 3:
                issues.append(f"ç±»åˆ«ä¸å¹³è¡¡ (æ¯”ä¾‹={imbalance_ratio:.2f})")
                recommendations.append(
                    "å»ºè®®2: ä½¿ç”¨åŠ æƒé‡‡æ ·æˆ–è¿‡é‡‡æ ·/æ¬ é‡‡æ ·"
                )
                print(f"  âš ï¸  ç±»åˆ«ä¸å¹³è¡¡: æœ€å¤§/æœ€å°æ¯”ä¾‹={imbalance_ratio:.2f}")
            else:
                print(f"  âœ“ ç±»åˆ«åˆ†å¸ƒå¹³è¡¡")
        
        # æ£€æŸ¥3: å›¾åƒç»Ÿè®¡é‡ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        import numpy as np
        sample_images = []
        for i in range(min(100, n_samples)):
            img, _ = dataset[i]
            if isinstance(img, torch.Tensor):
                sample_images.append(img.numpy())
        
        if sample_images:
            sample_array = np.array(sample_images)
            mean = sample_array.mean(axis=(0, 2, 3))
            std = sample_array.std(axis=(0, 2, 3))
            
            print(f"âœ“ å›¾åƒç»Ÿè®¡é‡ï¼ˆé‡‡æ ·{len(sample_images)}å¼ ï¼‰:")
            print(f"  - å‡å€¼: [{mean[0]:.4f}, {mean[1]:.4f}, {mean[2]:.4f}]")
            print(f"  - æ ‡å‡†å·®: [{std[0]:.4f}, {std[1]:.4f}, {std[2]:.4f}]")
        
        # æ€»ç»“
        print("\n" + "-"*60)
        if len(issues) == 0:
            print("âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ")
        else:
            print(f"âš ï¸  å‘ç° {len(issues)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for issue in issues:
                print(f"   â€¢ {issue}")
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in recommendations:
                print(f"   â€¢ {rec}")
        print("="*60 + "\n")
        
        return {
            'n_samples': n_samples,
            'issues': issues,
            'recommendations': recommendations
        }


# ============================================================================
# è®­ç»ƒè¯Šæ–­å™¨ï¼ˆè®­ç»ƒåå“åº”å¼æ£€æŸ¥ï¼‰
# ============================================================================

class TrainingDiagnostic:
    """è®­ç»ƒé—®é¢˜è‡ªåŠ¨è¯Šæ–­å™¨ï¼ˆå¯¹åº”æ•™ç¨‹è®­ç»ƒé—®é¢˜è¯Šæ–­è¡¨ï¼‰"""
    
    def __init__(self):
        self.history = []
        self.all_problems = []
        self.all_recommendations = []
    
    def add_epoch(self, metrics: dict):
        """æ·»åŠ ä¸€ä¸ªepochçš„è®°å½•"""
        self.history.append(metrics)
    
    def diagnose_current_state(self, epoch: int) -> Tuple[List[str], List[str]]:
        """è¯Šæ–­å½“å‰è®­ç»ƒçŠ¶æ€ï¼Œè¿”å›(é—®é¢˜åˆ—è¡¨, å»ºè®®åˆ—è¡¨)"""
        problems = []
        recommendations = []
        
        if len(self.history) == 0:
            return problems, recommendations
        
        last = self.history[-1]
        
        # é—®é¢˜1: Lossä¸ä¸‹é™/ä¸æ”¶æ•›ï¼ˆ5ä¸ªepochålosså˜åŒ–<1%ï¼‰
        if self._check_convergence_stall():
            problems.append("Lossä¸ä¸‹é™/ä¸æ”¶æ•›")
            recommendations.append("â˜… å»ºè®®: å­¦ä¹ ç‡é™è‡³0.1å€ï¼ˆå¦‚1e-3â†’1e-4ï¼‰")
        
        # é—®é¢˜2: è¿‡æ‹Ÿåˆä¸¥é‡ï¼ˆtrain-val gap>30%ï¼‰
        if self._check_overfitting():
            problems.append("è¿‡æ‹Ÿåˆä¸¥é‡")
            gap = last['train_acc'] - last['val_acc']
            recommendations.append(
                f"â˜… å»ºè®®: L2æ­£åˆ™åŒ–(weight_decay=1e-4)æˆ–æ•°æ®å¢å¼º (å½“å‰gap={gap:.1%})"
            )
        
        # é—®é¢˜3: Losséœ‡è¡/ä¸ç¨³å®šï¼ˆç›¸é‚»epochæ³¢åŠ¨>20%ï¼‰
        if self._check_instability():
            problems.append("Losséœ‡è¡/ä¸ç¨³å®š")
            recommendations.append("â˜… å»ºè®®: å­¦ä¹ ç‡é™è‡³0.5å€ï¼ˆå¦‚1e-3â†’5e-4ï¼‰")
        
        # é—®é¢˜4: æ”¶æ•›å¤ªæ…¢ï¼ˆ10ä¸ªepochåval_accå¢é•¿<5%ï¼‰
        if self._check_slow_convergence():
            problems.append("æ”¶æ•›å¤ªæ…¢")
            recommendations.append("â˜… å»ºè®®: å­¦ä¹ ç‡å¢è‡³2å€ï¼ˆå¦‚1e-4â†’2e-4ï¼‰")
        
        # é—®é¢˜5: æ¬ æ‹Ÿåˆï¼ˆtrain_acc<70% ä¸” val_acc<65%ï¼‰
        if self._check_underfitting():
            problems.append("æ¬ æ‹Ÿåˆ")
            recommendations.append(
                "â˜… å»ºè®®: å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆå±‚æ•°+2æˆ–é€šé“æ•°ç¿»å€ï¼‰æˆ–æ£€æŸ¥æ•°æ®é‡"
            )
        
        # é—®é¢˜6: NaN/Inf
        if self._check_nan_inf(last):
            problems.append("å‡ºç°NaN/Inf")
            recommendations.append("â˜… å»ºè®®: å­¦ä¹ ç‡é™è‡³1e-5æˆ–ä½¿ç”¨æ¢¯åº¦è£å‰ª")
        
        return problems, recommendations
    
    def _check_convergence_stall(self) -> bool:
        """æ£€æŸ¥ï¼š5ä¸ªepochålosså˜åŒ–<1%"""
        if len(self.history) < 5:
            return False
        recent_losses = [h['train_loss'] for h in self.history[-5:]]
        max_loss = max(recent_losses)
        min_loss = min(recent_losses)
        if max_loss < 1e-8:
            return False
        loss_change = (max_loss - min_loss) / max_loss
        return loss_change < 0.01
    
    def _check_overfitting(self) -> bool:
        """æ£€æŸ¥ï¼štrain-val gap>30%"""
        if not self.history:
            return False
        last = self.history[-1]
        gap = last['train_acc'] - last['val_acc']
        return gap > 0.30
    
    def _check_instability(self) -> bool:
        """æ£€æŸ¥ï¼šç›¸é‚»epochæ³¢åŠ¨>20%"""
        if len(self.history) < 2:
            return False
        recent = self.history[-2:]
        loss_change = abs(recent[1]['train_loss'] - recent[0]['train_loss'])
        if recent[0]['train_loss'] < 1e-8:
            return False
        volatility = loss_change / recent[0]['train_loss']
        return volatility > 0.20
    
    def _check_slow_convergence(self) -> bool:
        """æ£€æŸ¥ï¼š10ä¸ªepochåval_accå¢é•¿<5%"""
        if len(self.history) < 10:
            return False
        acc_gain = self.history[-1]['val_acc'] - self.history[-10]['val_acc']
        return acc_gain < 0.05
    
    def _check_underfitting(self) -> bool:
        """æ£€æŸ¥ï¼štrain_acc<70% ä¸” val_acc<65%"""
        if not self.history:
            return False
        last = self.history[-1]
        return last['train_acc'] < 0.70 and last['val_acc'] < 0.65
    
    def _check_nan_inf(self, metrics: dict) -> bool:
        """æ£€æŸ¥ï¼šæ˜¯å¦å‡ºç°NaN/Inf"""
        import math
        for v in metrics.values():
            if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                return True
        return False
    
    def print_diagnosis(self, epoch: int):
        """æ‰“å°è¯Šæ–­ç»“æœ"""
        problems, recommendations = self.diagnose_current_state(epoch)
        
        if problems:
            print("\n" + "="*60)
            print(f"âš ï¸  è®­ç»ƒè¯Šæ–­æŠ¥å‘Š (Epoch {epoch})")
            print("="*60)
            print(f"æ£€æµ‹åˆ° {len(problems)} ä¸ªé—®é¢˜:")
            for i, prob in enumerate(problems, 1):
                print(f"  {i}. {prob}")
            
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in recommendations:
                print(f"  {rec}")
            print("="*60 + "\n")
            
            # è®°å½•æ‰€æœ‰é—®é¢˜å’Œå»ºè®®ï¼ˆç”¨äºæœ€ç»ˆæŠ¥å‘Šï¼‰
            for p in problems:
                if p not in self.all_problems:
                    self.all_problems.append(p)
            for r in recommendations:
                if r not in self.all_recommendations:
                    self.all_recommendations.append(r)


# ============================================================================
# è¯Šæ–­ä»ªè¡¨ç›˜å¯è§†åŒ–
# ============================================================================

class TrainingDashboard:
    """å®æ—¶è®­ç»ƒè¯Šæ–­ä»ªè¡¨ç›˜"""
    
    @staticmethod
    def plot_diagnostic_dashboard(history: List[dict], output_path: str):
        """ç”Ÿæˆè¯Šæ–­ä»ªè¡¨ç›˜"""
        try:
            import matplotlib
            matplotlib.use("Agg")
            import matplotlib.pyplot as plt
        except ImportError:
            print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ä»ªè¡¨ç›˜ç”Ÿæˆ")
            return
        
        if not history:
            return
        
        fig = plt.figure(figsize=(16, 10))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        epochs = [h['epoch'] for h in history]
        train_loss = [h['train_loss'] for h in history]
        val_loss = [h['val_loss'] for h in history]
        train_acc = [h['train_acc'] for h in history]
        val_acc = [h['val_acc'] for h in history]
        
        # 1. Lossæ›²çº¿
        ax1 = fig.add_subplot(gs[0, :2])
        ax1.plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)
        ax1.plot(epochs, val_loss, 'r-', label='Val Loss', linewidth=2)
        ax1.set_title('Loss Curves', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å‡†ç¡®ç‡æ›²çº¿
        ax2 = fig.add_subplot(gs[1, :2])
        ax2.plot(epochs, train_acc, 'b-', label='Train Acc', linewidth=2)
        ax2.plot(epochs, val_acc, 'r-', label='Val Acc', linewidth=2)
        ax2.set_title('Accuracy Curves', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è¿‡æ‹ŸåˆæŒ‡æ ‡
        ax3 = fig.add_subplot(gs[0, 2])
        overfitting = [h['train_acc'] - h['val_acc'] for h in history]
        ax3.plot(epochs, overfitting, 'orange', linewidth=2)
        ax3.axhline(y=0.30, color='r', linestyle='--', label='Threshold (30%)', linewidth=1)
        ax3.set_title('Overfitting Gap', fontsize=12)
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Train-Val Gap')
        ax3.legend(fontsize=9)
        ax3.grid(True, alpha=0.3)
        
        # 4. Lossç¨³å®šæ€§
        ax4 = fig.add_subplot(gs[1, 2])
        if len(train_loss) > 1:
            loss_volatility = [
                abs(train_loss[i] - train_loss[i-1]) / max(train_loss[i-1], 1e-8)
                for i in range(1, len(train_loss))
            ]
            ax4.plot(epochs[1:], loss_volatility, 'purple', linewidth=2)
            ax4.axhline(y=0.20, color='r', linestyle='--', label='Threshold (20%)', linewidth=1)
        ax4.set_title('Loss Volatility', fontsize=12)
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Relative Change')
        ax4.legend(fontsize=9)
        ax4.grid(True, alpha=0.3)
        
        # 5. è¯Šæ–­æ–‡æœ¬æ¡†
        ax5 = fig.add_subplot(gs[2, :])
        ax5.axis('off')
        
        # è‡ªåŠ¨è¯Šæ–­
        last = history[-1]
        diagnosis_text = "ğŸ” è‡ªåŠ¨è¯Šæ–­ç»“æœ:\n\n"
        
        if last['train_acc'] - last['val_acc'] > 0.30:
            gap = last['train_acc'] - last['val_acc']
            diagnosis_text += f"âš ï¸  è¿‡æ‹Ÿåˆä¸¥é‡ (gap={gap:.1%} > 30%)\n"
            diagnosis_text += "   â†’ å»ºè®®ï¼šå¢åŠ weight_decayæˆ–æ•°æ®å¢å¼º\n\n"
        
        if len(history) >= 5:
            recent_losses = [h['train_loss'] for h in history[-5:]]
            max_loss = max(recent_losses)
            min_loss = min(recent_losses)
            if max_loss > 1e-8:
                change_rate = (max_loss - min_loss) / max_loss
                if change_rate < 0.01:
                    diagnosis_text += f"âš ï¸  Lossä¸æ”¶æ•› (5 epochå˜åŒ–={change_rate:.1%} < 1%)\n"
                    diagnosis_text += "   â†’ å»ºè®®ï¼šé™ä½å­¦ä¹ ç‡è‡³0.1å€\n\n"
        
        if last['train_acc'] < 0.70 and last['val_acc'] < 0.65:
            diagnosis_text += "âš ï¸  æ¬ æ‹Ÿåˆ\n"
            diagnosis_text += "   â†’ å»ºè®®ï¼šå¢åŠ æ¨¡å‹å®¹é‡æˆ–æé«˜å­¦ä¹ ç‡\n\n"
        
        if "âš ï¸" not in diagnosis_text:
            diagnosis_text += "âœ… è®­ç»ƒçŠ¶æ€æ­£å¸¸\n"
        
        ax5.text(0.05, 0.5, diagnosis_text, 
                fontsize=11, verticalalignment='center',
                fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
        
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"âœ“ ä¿å­˜è¯Šæ–­ä»ªè¡¨ç›˜: {output_path}")


# ============================================================================
# è®­ç»ƒä¸è¯„ä¼°å‡½æ•°
# ============================================================================

def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    for images, labels in loader:
        images = images.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
    avg_loss = total_loss / len(loader)
    acc = correct / max(total, 1)
    return avg_loss, acc


def eval_one_epoch(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    avg_loss = total_loss / len(loader)
    acc = correct / max(total, 1)
    return avg_loss, acc


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def _ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def _count_params(model: nn.Module) -> int:
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def _set_seed(seed: int) -> None:
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def _cifar10_transforms(data_augment: bool) -> Tuple[transforms.Compose, transforms.Compose]:
    mean = (0.4914, 0.4822, 0.4465)
    std = (0.2470, 0.2435, 0.2616)

    train_tfms: List[transforms.Compose] = []
    if data_augment:
        train_tfms.extend([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
        ])
    train_tfms.extend([transforms.ToTensor(), transforms.Normalize(mean, std)])
    test_tfms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])
    return transforms.Compose(train_tfms), test_tfms


def _save_train_log_csv(rows: List[EpochRow], out_path: str) -> None:
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["epoch", "train_loss", "train_acc", "val_loss", "val_acc"],
        )
        writer.writeheader()
        for row in rows:
            writer.writerow(asdict(row))


def _plot_training(rows: List[EpochRow], out_path: str) -> None:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt

    epochs = [r.epoch for r in rows]
    train_loss = [r.train_loss for r in rows]
    val_loss = [r.val_loss for r in rows]
    train_acc = [r.train_acc for r in rows]
    val_acc = [r.val_acc for r in rows]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4.5))
    ax1.plot(epochs, train_loss, label="train_loss", linewidth=2)
    ax1.plot(epochs, val_loss, label="val_loss", linewidth=2)
    ax1.set_xlabel("Epoch")
    ax1.set_title("Loss")
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    ax2.plot(epochs, train_acc, label="train_acc", linewidth=2)
    ax2.plot(epochs, val_acc, label="val_acc", linewidth=2)
    ax2.set_xlabel("Epoch")
    ax2.set_title("Accuracy")
    ax2.set_ylim([0, 1])
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close(fig)


# ============================================================================
# æ™ºèƒ½è®­ç»ƒä¸»å‡½æ•°
# ============================================================================

def run_experiment_with_diagnosis(
    *,
    name: str,
    model: nn.Module,
    train_loader: DataLoader,
    test_loader: DataLoader,
    train_dataset,
    device: torch.device,
    out_dir: str,
    num_epochs: int,
    lr: float,
    weight_decay: float,
    data_augment: bool,
    train_subset: int,
    test_subset: int,
    batch_size: int,
) -> Dict:
    """å¸¦æ™ºèƒ½è¯Šæ–­çš„è®­ç»ƒå‡½æ•°"""
    _ensure_dir(out_dir)
    
    # è®­ç»ƒå‰æ•°æ®è´¨é‡æ£€æŸ¥
    data_quality = DataQualityChecker.check_data_quality(train_dataset)
    
    # åˆå§‹åŒ–è¯Šæ–­å™¨
    diagnostic = TrainingDiagnostic()
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    history: List[EpochRow] = []
    start = time.time()

    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {name}")
    print(f"{'='*60}")
    print(f"æ¨¡å‹å‚æ•°é‡: {_count_params(model):,}")
    print(f"è®­ç»ƒæ ·æœ¬: {train_subset}, éªŒè¯æ ·æœ¬: {test_subset}")
    print(f"å­¦ä¹ ç‡: {lr}, Weight Decay: {weight_decay}, æ•°æ®å¢å¼º: {data_augment}")
    print(f"{'='*60}\n")
    
    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc = eval_one_epoch(model, test_loader, criterion, device)
        
        epoch_row = EpochRow(
            epoch=epoch + 1,
            train_loss=round(train_loss, 6),
            train_acc=round(train_acc, 6),
            val_loss=round(val_loss, 6),
            val_acc=round(val_acc, 6),
        )
        history.append(epoch_row)
        
        # æ·»åŠ åˆ°è¯Šæ–­å™¨
        diagnostic.add_epoch({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc
        })
        
        # æ‰“å°è¿›åº¦
        gap = train_acc - val_acc
        print(
            f"Epoch {epoch+1:2d}/{num_epochs} | "
            f"train_acc: {train_acc:.4f}, val_acc: {val_acc:.4f} | "
            f"gap: {gap:+.1%}"
        )
        
        # æ¯3ä¸ªepochè¿›è¡Œä¸€æ¬¡è¯Šæ–­ï¼ˆé¿å…è¿‡äºé¢‘ç¹ï¼‰
        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:
            diagnostic.print_diagnosis(epoch + 1)

    elapsed = time.time() - start
    num_params = _count_params(model)

    # ä¿å­˜æ ‡å‡†è¾“å‡º
    train_log_path = os.path.join(out_dir, "train_log.csv")
    model_path = os.path.join(out_dir, "model.pth")
    plot_path = os.path.join(out_dir, "training_plot.png")
    dashboard_path = os.path.join(out_dir, "diagnostic_dashboard.png")
    metrics_path = os.path.join(out_dir, "metrics.json")

    _save_train_log_csv(history, train_log_path)
    torch.save(model.state_dict(), model_path)
    _plot_training(history, plot_path)
    
    # ç”Ÿæˆè¯Šæ–­ä»ªè¡¨ç›˜
    TrainingDashboard.plot_diagnostic_dashboard(diagnostic.history, dashboard_path)

    last = history[-1]
    metrics = RunMetrics(
        model=name,
        num_params=num_params,
        device=str(device),
        num_epochs=num_epochs,
        train_subset=train_subset,
        test_subset=test_subset,
        batch_size=batch_size,
        lr=lr,
        weight_decay=weight_decay,
        data_augment=data_augment,
        elapsed_sec=round(elapsed, 2),
        final_train_acc=last.train_acc,
        final_val_acc=last.val_acc,
        final_train_loss=last.train_loss,
        final_val_loss=last.val_loss,
        diagnosed_problems=diagnostic.all_problems,
        recommendations=diagnostic.all_recommendations,
    )
    
    with open(metrics_path, "w", encoding="utf-8") as f:
        json.dump(asdict(metrics), f, indent=2, ensure_ascii=False)

    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"æ€»ç”¨æ—¶: {elapsed:.2f}ç§’")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {last.train_acc:.4f}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {last.val_acc:.4f}")
    print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {(last.train_acc - last.val_acc):.1%}")
    
    if diagnostic.all_problems:
        print(f"\nâš ï¸  è®­ç»ƒè¿‡ç¨‹ä¸­æ£€æµ‹åˆ°çš„é—®é¢˜:")
        for prob in diagnostic.all_problems:
            print(f"   â€¢ {prob}")
        print(f"\nğŸ’¡ æ€»ä½“æ”¹è¿›å»ºè®®:")
        for rec in diagnostic.all_recommendations:
            print(f"   â€¢ {rec}")
    else:
        print("\nâœ… è®­ç»ƒè¿‡ç¨‹æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜")
    
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   â€¢ è®­ç»ƒæ—¥å¿—: {train_log_path}")
    print(f"   â€¢ æ¨¡å‹æƒé‡: {model_path}")
    print(f"   â€¢ è®­ç»ƒæ›²çº¿: {plot_path}")
    print(f"   â€¢ è¯Šæ–­ä»ªè¡¨ç›˜: {dashboard_path}")
    print(f"   â€¢ æŒ‡æ ‡æ‘˜è¦: {metrics_path}")
    print("="*60 + "\n")

    return {
        "out_dir": out_dir,
        "train_log": train_log_path,
        "model_path": model_path,
        "training_plot": plot_path,
        "diagnostic_dashboard": dashboard_path,
        "metrics": metrics_path,
        "metrics_obj": asdict(metrics),
        "history": [asdict(r) for r in history],
        "data_quality": data_quality,
    }


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="CIFAR-10 æ™ºèƒ½è¯Šæ–­è®­ç»ƒï¼ˆé˜¶æ®µ1ï¼šè¯Šæ–­+å»ºè®®ï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="simple_cnn",
        choices=["simple_cnn", "resnet18", "both"],
        help="Which model to train",
    )
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--subset", type=int, default=5000, help="Train subset size")
    parser.add_argument("--test_subset", type=int, default=1000, help="Test subset size")
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=0.0)
    parser.add_argument(
        "--data_augment",
        type=str,
        default="False",
        help="True/False. Enable basic data augmentation",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="Output directory for this experiment",
    )
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    data_augment = str(args.data_augment).lower() in {"1", "true", "yes", "y"}

    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    default_out_root = os.path.join(base_dir, "tutorial_runs", "output")
    out_dir = args.output or os.path.join(default_out_root, f"{args.model}_smart")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    _set_seed(args.seed)

    train_tfm, test_tfm = _cifar10_transforms(data_augment=data_augment)
    data_root = os.path.join(base_dir, "tutorial_runs", "data")
    train_ds = torchvision.datasets.CIFAR10(
        root=data_root,
        train=True,
        download=True,
        transform=train_tfm,
    )
    test_ds = torchvision.datasets.CIFAR10(
        root=data_root,
        train=False,
        download=True,
        transform=test_tfm,
    )

    train_subset = Subset(train_ds, list(range(min(args.subset, len(train_ds)))))
    test_subset = Subset(test_ds, list(range(min(args.test_subset, len(test_ds)))))
    train_loader = DataLoader(train_subset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    test_loader = DataLoader(test_subset, batch_size=args.batch_size, shuffle=False, num_workers=0)

    def _make_model(model_name: str) -> nn.Module:
        if model_name == "simple_cnn":
            return SimpleCNN(num_classes=10)
        if model_name == "resnet18":
            return torchvision.models.resnet18(weights=None, num_classes=10)
        raise ValueError(f"Unknown model: {model_name}")

    runs: List[Dict] = []
    models_to_run: List[str]
    if args.model == "both":
        models_to_run = ["simple_cnn", "resnet18"]
    else:
        models_to_run = [args.model]

    for model_name in models_to_run:
        this_out = out_dir
        if args.model == "both":
            this_out = os.path.join(out_dir, model_name)

        model = _make_model(model_name).to(device)
        run = run_experiment_with_diagnosis(
            name=model_name,
            model=model,
            train_loader=train_loader,
            test_loader=test_loader,
            train_dataset=train_subset,
            device=device,
            out_dir=this_out,
            num_epochs=args.epochs,
            lr=args.lr,
            weight_decay=args.weight_decay,
            data_augment=data_augment,
            train_subset=len(train_subset),
            test_subset=len(test_subset),
            batch_size=args.batch_size,
        )
        runs.append(run)

    # å¦‚æœè¿è¡Œå¤šä¸ªæ¨¡å‹ï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if len(runs) > 1:
        _ensure_dir(default_out_root)
        summary_path = os.path.join(default_out_root, "smart_training_summary.json")
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "model": r["metrics_obj"]["model"],
                        "metrics": r["metrics_obj"],
                        "data_quality": r["data_quality"],
                        "out_dir": r["out_dir"],
                    }
                    for r in runs
                ],
                f,
                indent=2,
                ensure_ascii=False,
            )
        print(f"âœ“ å¯¹æ¯”æ‘˜è¦å·²ä¿å­˜: {summary_path}")


if __name__ == "__main__":
    main()
