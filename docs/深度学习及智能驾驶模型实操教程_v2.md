# 深度学习及智能驾驶模型实操教程（学习手册版 v2）

---

## 使用说明

这是一份可顺序阅读的学习手册。每章包含概念讲解、操作步骤与检查点，读完即可完成最小闭环。你可以直接将每章小节改写为 PPT 讲义。

推荐阅读顺序：1 -> 2 -> 3 -> 4

---
参考资源：  
- PyTorch Tutorial：https://github.com/yunjey/pytorch-tutorial  
- Deep Learning Book（中文）：https://github.com/exacity/deeplearningbook-chinese  
- Awesome DL 资源索引：https://github.com/ChristosChristofidis/awesome-deep-learning  

---

## 目录

1. 掌握深度学习基础（Day 1 上午）
2. 实操深度学习案例（Day 1 下午）
3. 探索智能驾驶模型（Day 2 上午）
4. 实操智能驾驶案例（Day 2 下午）

---

## 第 1 章 掌握深度学习基础（Day 1 上午）

### 引言：怎样守护我的职业未来？
问：AI会写代码、做算法，似乎无所不能，作为一名码农，我该怎么办？
答：拥有机器学习的完整知识体系，AI辅助下掌握全栈开发技能，成为理解AI的同行者。

### 1.1 WHAT：深度学习是什么？

#### 1.1.1 深度学习与经典机器学习的联系与区别
- 深度学习与经典机器学习的核心区别在于"特征提取方式"：
| 维度 | 经典机器学习（如 SVM、决策树） | 深度学习 |
| --- | --- | --- |
| 特征提取 | 需要人工设计特征（如 HOG、SIFT） | 自动学习特征表示（从原始数据到高层语义） |
| 学习内容 | 模型只学习"如何组合已有特征" | 端到端训练，减少人工设计环节 |
| 数据需求 | 适合小数据量 | 需要大量数据与计算资源 |
| 计算资源 | 计算需求较低 | 需要大量计算资源 |
| 可解释性 | 可解释性要求高的场景 | 可解释性相对较弱 |
| 适用场景 | 结构化数据、小样本场景 | 图像、语音、文本等高维（非结构化）数据上表现更优 |

- 深度学习与经典机器学习的联系是：
  - 都需要损失函数与优化器
  - 都面临过拟合与泛化问题
  - 深度学习可以视为"可学习特征"的机器学习扩展

#### 1.1.2 深度学习发展历史
| 年份 | 里程碑 | 发明人/团队 | 说明 |
| --- | --- | --- | --- |
| 1986 | 反向传播算法 | David Rumelhart, **Geoffrey Hinton**, Ronald Williams | 推动多层网络训练成为可能 |
| 1998 | LeNet | **Yann LeCun** 等 | 在手写数字识别中验证卷积网络有效性 |
| 2012 | AlexNet | Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton | 引爆深度学习视觉浪潮 |
| 2015 | ResNet | Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (MSRA) | 解决深层网络退化问题,成为视觉主干常用选择 |
| 2017 | Transformer | Ashish Vaswani 等 (Google) | 提出自注意力机制,变革序列建模 |
| 2020s | 多模态模型 | 多机构并行探索 (OpenAI CLIP, GPT-4V 等) | 推动视觉、语言、动作的统一建模 |
| 2025 | 3D 世界模型 | **李飞飞**团队 (World Labs) | 发布最新 3D 世界模型,推动具身智能与可预测世界表示 |

注：粗体为人工智能发展重要人物，将被重点讲解。

#### 1.1.3 深度学习三件套：模型 / 损失函数 / 优化器
深度学习可以理解为“可训练的函数”。它由三件套构成：模型、损失函数、优化算法。  
这三者决定了模型“能表达什么”“学什么目标”“怎么学”。

- 模型：线性层提供可学习的特征组合，但只能表示线性关系；激活函数（ReLU/Sigmoid/Tanh）引入非线性，才能学到复杂模式。  
- 损失函数：训练的目标不是“准确率”，而是最小化损失；分类任务常用交叉熵，回归任务常用 MSE。  
- 优化器：SGD 最基础稳定但可能收敛慢；Adam 适合新手、默认好用。  

### 1.2 WHY：为什么要学深度学习？

#### 1.2.1 深度学习在智能驾驶中的核心地位
深度学习在智能驾驶中扮演核心角色，主要体现在以下几个方面：
- 感知能力：深度学习驱动的视觉和点云处理模型（如 CNN、PointNet）是实现环境感知的基础。
- 预测与规划：深度学习模型（如 RNN、Transformer）用于预测其他交通参与者的行为，支持安全规划。
- 端到端学习：深度学习使得从传感器输入到控制输出的端到端学习成为可能，简化了系统架构。
- 多模态融合：深度学习支持多传感器数据（摄像头、雷达、激光雷达）的融合，提高感知的鲁棒性和准确性。
- 持续改进：通过大规模数据训练和在线学习，深度学习模型能够不断提升性能，适应复杂多变的驾驶环境。   

#### 1.2.2 深度学习的应用场景
在智能驾驶中，深度学习的主要应用场景及对应模型包括：
- 目标检测与识别：使用 CNN 模型（如 YOLO、Faster R-CNN）检测车辆、行人、交通标志等。
- 语义分割：利用深度学习实现道路、车道线、障碍物的像素级分类。
- 行为预测：通过 RNN 或 Transformer 模型预测其他车辆和行人的未来轨迹。
- 路径规划：深度强化学习用于生成安全、高效的行驶路径。
- 驾驶决策：端到端模型直接从传感器数据学习驾驶策略。
- 多传感器融合：结合摄像头、雷达和激光雷达数据，提高环境感知的准确性和鲁棒性。

### 1.3 HOW：如何搭建最小训练闭环（来自 PyTorch Tutorial）

#### 1.3.1 一个可复现的训练闭环  
- 数据准备（Dataset/Dataloader）  
- 模型定义（nn.Module）  
- 损失函数与优化器  
- 训练循环（forward -> loss -> backward -> step）  
- 评估与保存  

最小训练循环（示意）：  
```python
for images, labels in dataloader:
    outputs = model(images)
    loss = criterion(outputs, labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

理解这个闭环后，你就能在第 2 章跑通“CNN vs ResNet18”的实操对比。

#### 1.3.2 过拟合与泛化基础
训练并不等于“学会”。真正重要的是模型能否在新数据上表现稳定。  
因此你要理解“过拟合”和“泛化”的基本判断逻辑。

新手泛化判断规则：  
- 训练 loss 下降、验证 loss 同步下降：正常学习  
- 训练下降、验证不下降：可能过拟合  
- 训练与验证都不下降：学习率不合适或模型太弱  

避免过拟合的最小手段：  
- Dropout：训练时随机“关闭”部分神经元  
- Weight Decay：限制参数过大  
- 数据增强：用更多“视角”提升泛化  


## 第 2 章 实操深度学习案例（Day 1 下午）

本章基于 PyTorch Tutorial 的训练结构，完成“CNN vs ResNet18”对比实验。  
目标是把第 1 章的理论变成可执行的训练闭环。

### 2.1 你将学到什么
对于一名深度学习初学者来说，本章实操能帮助你达成以下可验证的目标：
- 能用一句话描述本实验的目标与数据（例如：在 CIFAR-10 子集上比较两种模型的泛化表现）。
- 能运行训练脚本并成功生成训练日志、模型权重与训练/验证曲线图。
- 能读懂并解释训练/验证的损失与准确率曲线，识别过拟合或欠拟合现象，并提出至少一种改进方案。
- 学会并能实际应用至少三种常用的泛化增强手段（如数据增强、Weight Decay、Dropout 或早停）。
- 能比较 Simple CNN 与 ResNet18 的训练时间、参数规模与测试性能，并解释差异产生的主要原因。
- 能在一次小实验中修改一个超参数（例如学习率或是否使用数据增强），记录并解读结果差异。

### 2.2 实验设计（来自 PyTorch Tutorial + CIFAR 实战范式）

#### 2.2.1 数据集：CIFAR-10
- **数据集介绍**：CIFAR-10 是计算机视觉领域的经典基准数据集
- **图像规格**：32×32 像素彩色图像（3 通道 RGB）
- **类别数量**：10 类（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车）
- **完整数据集规模**：训练集 50,000 张，测试集 10,000 张
- **本实验实际使用**：训练集 5,000 张（10%子集），测试集 1,000 张（10%子集）
- **使用子集的原因**：
  - 加速训练演示（CPU环境约3-5分钟完成）
  - 更明显地展示过拟合现象（小数据集更容易过拟合）
  - 模拟真实场景中数据量有限的情况
- **扩展实验**：可修改脚本使用完整数据集，观察性能变化
- 数据集下载/缓存路径：`tutorial_runs/data/`（`torchvision` 默认会生成 `cifar-10-batches-py/`）

**输入与目标变量**：
- **输入（X）**：32×32×3 的 RGB 图像张量，先 `ToTensor()` 转到 [0, 1]，再做标准化 `Normalize(mean, std)`
- **目标变量（Y）**：类别标签，取值范围 0-9，对应 10 个类别
  - 0: airplane（飞机）
  - 1: automobile（汽车）
  - 2: bird（鸟）
  - 3: cat（猫）
  - 4: deer（鹿）
  - 5: dog（狗）
  - 6: frog（青蛙）
  - 7: horse（马）
  - 8: ship（船）
  - 9: truck（卡车）
- **任务类型**：多分类问题（10 类单标签分类）
- **模型目标**：学习从图像到类别标签的映射函数 f: X → Y

#### 2.2.2 对比模型
**Simple CNN**（简单卷积网络）：
- 结构：2 个卷积层 + 2 个全连接层（脚本实现）
- 参数量：约 107 万个可训练参数（以脚本实际统计为准）
- 特点：结构简单、训练快速、适合入门理解

**ResNet18**（18 层残差网络）：
- 结构：18 层深度网络 + 残差连接（skip connections）
- 参数量：约 1118 万个可训练参数
- 特点：通过残差连接解决深层网络退化问题
，能有效学习更复杂的特征表示

**什么是"深层网络退化"？**

深层网络退化（Degradation Problem）是指当神经网络层数增加到一定程度后，训练误差和测试误差反而上升的现象。这与过拟合不同——过拟合是训练误差低但测试误差高，而退化是训练误差本身就开始上升。

**ResNet 的解决方案：**
- 引入残差连接（Skip Connections / Shortcut Connections）
- 将学习目标从 H(x) 改为学习残差 F(x) = H(x) - x
- 使得网络更容易学习恒等映射，至少不会比浅层网络差
- 公式：输出 = F(x) + x，其中 F(x) 是残差块学习的部分

**直观理解：**
如果某一层不需要学习新特征，残差连接可以让它直接"跳过"（F(x)=0），保持输入不变。这比让整个层学习恒等映射 H(x)=x 要容易得多。
- 优势：能学习更复杂的特征表示

**为什么对比这两个模型？**
- 代表"浅层简单网络"vs"深层残差网络"的典型差异
- 直观感受网络深度与结构设计对性能的影响

**对比的真实价值（基于一次可复现实验结果）**

使用本教程默认配置（训练集 5000、测试集 1000、Adam lr=1e-3、10 epoch、无数据增强、无 weight decay），我们得到了一个**非常适合教学的现象**：

说明：为简化流程与加速对比，本教程直接使用 CIFAR-10 的官方 test split 作为“验证/评估集”（即脚本日志中的 `val_loss/val_acc`）。全文后续统一使用“验证/评估集（val）”这一称呼。

| 维度 | Simple CNN | ResNet18 | 对比 |
|------|-----------|----------|------|
| 训练准确率 | 95.08% | 86.56% | Simple CNN 更高（更易“记住”训练集） |
| 验证/评估准确率（val） | 57.30% | 50.60% | Simple CNN 更高 6.7% |
| 过拟合程度（train-val） | 37.78% | 35.96% | 两者都明显过拟合（小数据典型现象） |
| 训练时间（CPU） | 21.16 秒 | 175.37 秒 | ResNet18 慢约 8.3 倍 |
| 参数量 | 107 万 | 1118 万 | 相差约 10.4 倍 |

**核心发现：更大的模型不一定泛化更好（小数据尤其如此）**

这个结果**不是失败，而是最好的教学案例**，它揭示了深度学习工程实践的核心原则：

1. **表达能力 ≠ 泛化能力**
   - 训练集上两个模型都能取得很高准确率（Simple CNN 95.08%，ResNet18 86.56%）
   - 但验证/评估准确率明显更低（57.30% vs 50.60%）
   - 说明：只看训练集会被“拟合能力”误导，泛化才是关键

2. **小数据集的残酷现实**
   - 5000 个训练样本对 1118 万参数的 ResNet18 来说非常吃紧
   - Simple CNN 约 107 万参数相对更“克制”，更容易在小数据上取得更好的验证表现
   - **数据量决定模型上限，不是反过来**

3. **算力成本的巨大差异**
   - ResNet18 训练时间是 Simple CNN 的约 8.3 倍
   - 在车载芯片等资源受限场景，这是生死攸关的考量
   - **工程价值 = 性能 ÷ 成本**

4. **智能驾驶的现实启示**
   - 特斯拉/小鹏等车企需要在算力、功耗、实时性间权衡
   - 不是所有场景都需要最大最深的模型
   - 关键在于找到"够用且高效"的最优点

**这个对比的真正价值**：
- ✅ 打破"大模型一定更好"的迷思
- ✅ 建立"数据-模型-算力"匹配的工程思维
- ✅ 理解过拟合比模型容量更需要关注
- ✅ 培养真实项目中的权衡决策能力

**改进建议（扩展实验）**：
如果要让 ResNet18 发挥优势，需要：
- 使用完整 CIFAR-10 数据集（50,000 样本）
- 添加正则化（Dropout、Weight Decay）
- 使用数据增强（RandomFlip、RandomCrop）
- 调整学习率策略（学习率衰减）

**结论**：在小数据场景下，Simple CNN 是更合理的选择。这正是工程实践的智慧。

#### 2.2.3 评价指标
- **val_acc**（验证/评估集准确率）：模型在验证/评估集上预测正确的比例（本教程使用 CIFAR-10 的 test split 作为验证/评估集）
- **计算公式**：正确预测样本数 / 总样本数
- **本实验预期**：小数据子集下不保证“更深就更好”，你要用曲线与指标来验证，而不是凭直觉。

#### 2.2.4 实验配置
- **脚本位置**：`tutorial_runs/cifar_cnn_resnet.py`
- **硬件需求**：CPU 可运行（约 3-5 分钟），GPU 推荐（约 30-60 秒）
- **训练轮数**：10 个 epoch（充分训练以观察过拟合现象）
- **数据规模**：训练集 5000 张，测试集 1000 张（子集，用于加速演示）
- **优化器**：Adam，学习率 1e-3
- **关键目标**：对比不同模型在相同数据量下的泛化能力

### 2.3 实操步骤（增强版，可验证且可重复）

下面的步骤按“准备 -> 运行基线 -> 验证输出 -> 小实验”顺序，帮助你完成前面列出的可验证目标。

1) 环境与依赖（只需执行一次）

```bash
python -m pip install --upgrade pip
python -m pip install torchvision matplotlib numpy pillow
```

2) 运行基线训练（生成日志、权重与曲线）

- 运行 Simple CNN 基线（示例）：

```bash
python tutorial_runs/cifar_cnn_resnet.py --model simple_cnn --epochs 10 --subset 5000 --output tutorial_runs/output/simple_cnn
```

- 运行 ResNet18 基线（示例）：

```bash
python tutorial_runs/cifar_cnn_resnet.py --model resnet18 --epochs 10 --subset 5000 --output tutorial_runs/output/resnet18
```

（说明：如果脚本不支持命令行参数，可在脚本内修改相应变量：`model`, `epochs`, `subset`, `output_dir`。）

3) 验证输出（对应可验证目标）

- 确认生成文件：
   - `tutorial_runs/output/<experiment>/train_log.csv`（或 `.json`）——包含每个 epoch 的 train/val loss 与 acc
   - `tutorial_runs/output/<experiment>/model.pth` —— 保存好的模型权重
   - `tutorial_runs/output/<experiment>/training_plot.png` —— 训练/验证曲线图

- 可验证检查点对应关系：
   - “一句话描述实验目标”：在实验文件夹创建 `README.txt`，写出一句话（例如：在 CIFAR-10 子集上比较 Simple CNN 与 ResNet18 的泛化）。
   - “运行并生成日志/权重/曲线”：确认上述三个文件存在且可打开。  
   - “读懂并解释曲线”：打开 `training_plot.png`，标注训练/验证曲线差距，判断过拟合/欠拟合并写在 `README.txt`。

4) 常用泛化增强手段（至少选三种并实践）

- 数据增强（在数据加载处加入）：`RandomHorizontalFlip`, `RandomCrop`, `ColorJitter`。
- Weight Decay（优化器参数）：例如 `weight_decay=1e-4`。
- Dropout（模型中间层）：例如在全连接层添加 `nn.Dropout(p=0.5)`。
- 早停（Early Stopping）：在验证 loss 不再下降时提前停止训练。

示例：用数据增强 + weight decay 运行一次：

```bash
python tutorial_runs/cifar_cnn_resnet.py --model simple_cnn --epochs 10 --subset 5000 --data_augment True --weight_decay 1e-4 --output tutorial_runs/output/simple_cnn_aug
```

5) 小实验（只改一个变量，记录并对比）

- 实验 A（基线）：Simple CNN，lr=1e-3，无数据增强。
- 实验 B（只改数据增强）：与 A 相同，但开启数据增强。
- 实验 C（只改学习率）：与 A 相同，但 lr=5e-4。

每次运行后：
- 把 `train_log.csv` 和 `training_plot.png` 保存到不同文件夹（例如 `experiment_A/`, `experiment_B/`），并在 `summary.csv` 记录最后 epoch 的 train_acc, val_acc, train_loss, val_loss, 训练时长、参数量。

6) 对比与结论写法（短模板，便于新人记录）

- 模型一句话描述：<写在 `README.txt`>
- 结果摘要（表格）：模型 | train_acc | val_acc | train_loss | val_loss | 训练时间
- 结论（1-2 句）：是否过拟合？如果是，推荐改进（例如：增加数据增强或增加 weight decay）。

7) 示例对比命令（快速复现）

```bash
# 基线
python tutorial_runs/cifar_cnn_resnet.py --model simple_cnn --epochs 10 --subset 5000 --output tutorial_runs/output/baseline
# 增强：加数据增强
python tutorial_runs/cifar_cnn_resnet.py --model simple_cnn --epochs 10 --subset 5000 --data_augment True --output tutorial_runs/output/aug
# 增强：加 weight decay
python tutorial_runs/cifar_cnn_resnet.py --model simple_cnn --epochs 10 --subset 5000 --weight_decay 1e-4 --output tutorial_runs/output/wd
```

小结：按上述步骤每一步都能被文件存在/图像可视化/summary 表格验证，即完成了所有“可验证目标”。

8) 关键步骤拆解（理解要点，对应“为什么这么做”）

- 检查点 1：数据（Data）——使用 `torchvision.datasets.CIFAR10` 加载数据，并在 `DataLoader` 中拿到一个 batch（确认输入形状与标签范围正确）。
   - 新手常见坑：只做了 `ToTensor()` 没做标准化/增强，或忘了 `model.train()` / `model.eval()` 切换导致结果不稳定。
- 检查点 2：模型（Model）——能成功实例化 Simple CNN / `torchvision.models.resnet18`，并打印参数量（理解“模型容量”差异）。
   - 新手常见坑：把 ResNet18 当成“默认更好”，忽略小数据下大模型更易过拟合与训练更慢。
- 检查点 3：训练（Train）——训练循环包含 forward -> loss -> backward -> step，并且每个 epoch 都输出 train/val 指标（loss/acc）。
   - 新手常见坑：忘记 `optimizer.zero_grad()` 或把 `loss.backward()` / `optimizer.step()` 顺序写错，导致不收敛。
- 检查点 4：评估（Eval）——在验证/评估集（本教程用 CIFAR-10 的 test split）统一评估并保存可复现产物（日志、权重、曲线图），用于横向对比实验。
   - 新手常见坑：只看训练集准确率或只看最后一个 batch 的指标，导致对“泛化能力”判断错误。

### 2.4 评估与可视化

**训练过程对比图**：
![训练过程对比](tutorial_runs/output/training_comparison.png)

**图表解读**：
- **左图（Loss）**：训练 loss 持续下降，但验证 loss 更高且波动，提示过拟合风险
- **右图（Accuracy）**：训练准确率显著高于验证准确率，差距越大代表越过拟合
- **关键观察（以本次运行结果为例）**：
   - Simple CNN：train_acc=95.08%，val_acc=57.30%，gap=37.78%
   - ResNet18：train_acc=86.56%，val_acc=50.60%，gap=35.96%

**从曲线看工程决策**：
- 如果只看训练集指标，会误以为“train_acc 更高就更好”
- 但验证/评估指标与曲线揭示真相：泛化能力更重要
- 这正是为什么需要验证集/测试集的原因

**可视化脚本**：`tutorial_runs/visualize_training.py`  

### 2.5 误差分析（从指标到样本）

- 抽取 5 张错分样本图像
- 记录真实类别与预测类别
- 用 1 句话总结常见错误模式

### 2.6 小型改进实验（只改一个变量）

- 数据增强（随机翻转）
- 训练轮数从 1 增加到 3
- 学习率从 1e-3 调整到 5e-4

产出：  
- 改动前后 val_acc 对比  
- 1 句改进结论  


## 第 3 章 探索智能驾驶模型（Day 2 上午）

### 3.1 你将学到什么

把智能驾驶前沿概念与本次实操闭环对齐，理解“真实系统中这些概念的位置”。

### 3.2 世界模型与智能驾驶（核心概念）

世界模型强调“从当前状态预测未来状态”。  
在智能驾驶中，没有预测就没有安全规划，因此世界模型是关键能力之一。

#### 3.2.1 最小世界模型定义
- 状态 s_t：当前世界状态表示（可由感知模型输出）
- 动作 a_t：车辆或系统动作
- 转移：预测 s_{t+1}（未来状态）

最小输入/输出定义：  
输入是连续 T 帧的状态表示，输出是下一时刻的状态或场景变化。

#### 3.2.2 智能驾驶中的直接映射
- 前车减速 -> 预测距离变化
- 行人横穿 -> 预测占用区域变化
- 弯道进弯 -> 预测车道曲率变化

本课程实操不强求复杂世界模型实现，但要求理解“为什么必须预测”。

#### 3.2.3 世界模型发展脉络（概念级）
- RSSM / Dreamer / PlaNet：用潜变量建模时序状态
- 模型式强化学习：先学世界，再做决策
- 李飞飞的世界模型倡议：推动“可生成、可预测”的世界表示

#### 3.2.4 与实操的对应

本课程第 2 章训练的 CNN/ResNet 可以视为“感知特征抽取器”。  
在世界模型任务中，你会将每一帧的特征作为状态表示，再去预测下一时刻状态。  
也就是说：第 2 章解决“看见什么”，为本章的“预测未来”提供输入基础。

检查点：
- 能用 2 句话解释世界状态与世界模型
- 能说清本课程的输入与输出
### 3.3 智能驾驶技术发展里程碑（历史视角）

#### 早期阶段：规则与传感器（1980s-2000s）
- **1980s**：基于规则的车道检测与障碍物识别（ALV项目，CMU NavLab - Ernst Dickmanns, Chuck Thorpe）
- **1995**：CMU NavLab 5 横穿美国（Dean Pomerleau - ALVINN神经网络）
- **2004-2007**：DARPA 挑战赛（Stanley - Sebastian Thrun, 斯坦福；Boss - Chris Urmson, CMU）

#### 深度学习进入视觉感知（2012-2016）
- **2012**：AlexNet（Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton）引爆深度学习
- **2015**：ResNet（Kaiming He 等，MSRA）残差连接解决深层退化
- **2015**：YOLO v1（Joseph Redmon）实时目标检测

#### 多传感器融合与 BEV 发展（2017-2020）
- **2019**：Lift-Splat-Shoot（Jonah Philion, Sanja Fidler）BEV 视角转换
- **2020**：DETR（Nicolas Carion, Facebook AI）基于 Transformer 的检测
- **2020**：Tesla AI Day 展示纯视觉 BEV 方案（Andrej Karpathy）

#### 预测与世界模型兴起（2018-2022）
- **2018**：World Models（David Ha, Jürgen Schmidhuber）强化学习中的世界模型
- **2019**：Dream to Control（Danijar Hafner）RSSM/Dreamer 系列
- **2022至今**：李飞飞团队提出 Embodied AI 与世界模型倡议，她创建的World lab于2025年发布最新3D世界模型。

#### 语义与多模态趋势（2021-2024）
- **2021**：CLIP（Alec Radford, OpenAI）视觉-语言对齐
- **2023**：GPT-4V（OpenAI）多模态大模型
- **2024**：VLA（Vision-Language-Action）端到端具身智能（多机构并行探索）

#### 工程化与闭环迭代（2016-至今）
- **2016**：Waymo 正式成立（John Krafcik CEO，源自 Google Self-Driving Car）
- **2019**：Tesla FSD Beta 开启影子模式数据闭环（Andrej Karpathy）
- **2020+**：国内厂商（小鹏、华为、理想）推进 NOA/城市领航辅助

### 3.4 概念对照（作用级，金字塔梳理）

先给出一条主线：从数据与感知开始，经过融合与表示，建立世界模型预测，再进入决策与工程落地，最终服务于“可预测世界状态”的目标。  
实施顺序为：数据/基准层 -> 感知/检测层 -> 融合层 -> 表示层 -> 预测层 -> 语义层 -> 决策层 -> 工程层 -> 目标层。

**分层架构图（概念 vs 技术栈，一表读懂）**

| 分类 | 概念层级（作用） | 技术栈示例（非唯一） |
| --- | --- | --- |
| 基础与保障 | 数据/基准层：训练与验证数据 | **BehavioralCloningTrackData** / **CIFAR-10** |
| 感知与表示 | 感知/检测层：看见什么 | YOLO / Faster R-CNN / BEVDet / **ResNet18** |
| 感知与表示 | 融合层：如何对齐信息 | 多视角融合 / 时序对齐 / 卡尔曼滤波 |
| 感知与表示 | 表示层：怎么表示世界 | BEV Encoder / Occupancy / Feature Grid |
| 预测与决策 | 预测层：如何看未来 | RSSM / Dreamer / PlaNet / Simple CNN |
| 感知与表示 | 语义层：如何解释与约束 | **CLIP / VLM / VLA** |
| 预测与决策 | 决策层：如何使用结果 | 规划器 / 策略网络 / MPC |
| 基础与保障 | 工程层：训练->推理->导出 | **PyTorch** / **torchvision** / TensorFlow / Keras / ONNX / TensorRT |
| 基础与保障 | 目标层：统一可预测世界状态 | 表征学习 / 系统建模框架 |

注：教案提供实操案例的技术栈示例以加粗表示。

#### 3.4.1 感知融合

感知融合的核心是把多传感器信息对齐到同一坐标与时间轴，例如相机图像、激光雷达点云和车辆状态。它的重要性在于，只有对齐后，系统才能形成统一的世界状态表示。在本课程中，你可以把它理解为“预处理与特征对齐”的概念背景。

#### 3.4.2 BEV/3D

BEV/3D 是把场景从俯视或三维空间角度表达出来的方式。它的重要性在于提供结构化的空间理解，便于做预测与规划。在本课程中，它主要用于解释你生成的可视化结果应该具有什么样的空间直觉。

扩展：李飞飞倡导的“世界模型”（World Model / World Lab）旨在学习通用、可预测的三维世界表示，这与 BEV/3D 是互补且相辅相成的。

#### 3.4.3 世界模型时序建模

世界模型时序建模的目标是预测状态随时间变化的规律。它的重要性在于让系统不仅“看到当前”，还能“预见未来”。本课程的核心实操就是这一点：用前 T 帧预测下一帧。

#### 3.4.4 VLM/VLA

VLM/VLA 是视觉与语言结合的模型方向。它的重要性在于引入语义约束与可解释性，让系统理解高层目标或指令。本课程只做概念理解，不做实现。

#### 3.4.5 CLIP

CLIP 是视觉-语言对齐的代表模型，让模型学会“图像与文本如何对应”。在智能驾驶中可用于高层语义理解与提示。本课程只做概念理解，不做实现。

#### 3.4.6 状态驱动控制

状态驱动控制强调“控制应消费世界状态，而不是原始感知结果”。它的重要性在于系统组织结构更清晰。在本课程中，你只需要理解预测结果在真实系统中的位置与价值。

#### 3.4.7 工程化流程

工程化流程强调训练、推理、导出的可复现链路。它的重要性在于把模型从“能跑通”变成“可交付”。本课程已经提供最小闭环流程，作为工程化意识的入门。

：
- 能把至少 3 个概念映射到实操环节

### 3.5 技术路线对比（概念级，呼应 3.4）

以下对比用于帮助建立“路线分类”概念，不涉及商业评价或性能结论。你可以将其理解为不同系统组织方式的选择。

- 传统模块化方案（感知/预测/规划分工）：可解释性强、工程边界清晰，便于独立验证与替换。  
- 多传感器融合路线（相机+激光雷达+雷达）：强调冗余与稳定性，工程复杂度与成本更高。  
- 纯视觉路线：强调软件闭环与规模化，依赖更强的数据与训练策略。  
- 端到端倾向路线：强调从感知到控制的统一学习，依赖大规模数据闭环与持续迭代。  

典型路线示例（非结论性）：  
- Waymo - Waymo Driver：多传感器融合路线代表。  
- Cruise - Cruise AV Stack：多传感器融合路线代表。  
- 小鹏 - XNGP：融合路线与工程化落地并行推进。  
- 华为 - ADS：融合路线与工程化落地并行推进。  
- 理想 - AD：融合路线与工程化落地并行推进。  
- 特斯拉 - FSD：纯视觉与端到端倾向路线代表。  

---

## 第 4 章 实操智能驾驶案例（Day 2 下午）

### 4.1 你将学到什么

本章完成“行为克隆”从数据准备、训练到评估的完整闭环，  
对应智能驾驶的最小端到端落地实操。

### 4.2 数据与环境准备

#### 4.2.1 数据集介绍：Udacity Behavioral Cloning Track Data
**为什么选择该数据集作为实操案例：**
- 真实驾驶数据：数据来源于 Udacity 自动驾驶模拟器，包含真实驾驶场景
- 任务明确：行为克隆任务清晰，适合入门级深度学习实操
- 数据量适中：完整数据集约 8000 条记录，适合快速实验
- 多模态数据：包含图像与车辆状态，便于理解多模态输入
- 开源可用：数据集公开，易于获取与使用

**数据集来源**：
- 原始项目：Udacity 自动驾驶工程师纳米学位课程 - 行为克隆项目
- 公开镜像：https://github.com/KansaiUser/BehavioralCloningTrackData
- 数据采集方式：使用 Udacity 模拟器在赛道上人工驾驶录制

**数据集规模**：
- 完整数据集：约 8,000+ 条记录
- 本实验使用：随机采样 1,000 条（约 12.5% 子集）
- 数据类型：图像 + 转向角度 + 车辆状态
- 数据存放路径：`tutorial_runs/behavioral_cloning_data/`

**数据结构**：
每条记录包含以下信息：
- **中心摄像头图像**：`center_*.jpg`（主要使用）
- **左侧摄像头图像**：`left_*.jpg`（可选，用于数据增强）
- **右侧摄像头图像**：`right_*.jpg`（可选，用于数据增强）
- **转向角度**：`steering_angle`（连续值，范围 -1.0 到 +1.0）
- **油门**：`throttle`（0.0 到 1.0）
- **刹车**：`brake`（0.0 到 1.0）
- **车速**：`speed`（mph）

**数据视频预览**：

以下可视化展示了数据集的关键特征，帮助理解训练数据的分布与时序特性：

1. **样本图像与转向角度对应关系**  
    ![Driving Data Samples](tutorial_runs/output/driving_data_samples.png)  
    展示 15 个随机采样图像及其对应的转向角度值，直观理解图像-转向的映射关系。

2. **转向角度分布分析**  
    ![Driving Data Distribution](tutorial_runs/output/driving_data_distribution.png)  
    - 左图：转向角度直方图，显示数据集的转向角度分布特征
    - 右图：转向角度时序变化，展示驾驶过程中转向的动态模式

3. **连续帧序列可视化**  
    ![Driving Sequence Frames](tutorial_runs/output/driving_sequence_frames.png)  
    展示连续 8 帧图像序列，体现驾驶过程的时序连贯性与场景变化。

4. **转向模式时序分析**  
    ![Driving Timeline](tutorial_runs/output/driving_timeline.png)  
    分析前 500 帧的转向角度变化趋势，识别直行、左转、右转等驾驶模式。

5. **驾驶场景动态演示（GIF动画）**  
    ![Driving Demo GIF](tutorial_runs/output/driving_demo.gif)  
    动态展示实际驾驶场景，包含摄像头实时画面、转向角度数值、转向方向指示和转向强度可视化。GIF包含100帧，展示10秒的连续驾驶过程，自动循环播放。

**可视化脚本**：
- 静态图表生成：`tutorial_runs/visualize_driving_data.py`
- GIF动画生成：`tutorial_runs/create_driving_gif.py`

**输入与目标变量**：
- **输入（X）**：中心摄像头 RGB 图像（320×160 像素，调整为 200×66）
- **目标变量（Y）**：转向角度（浮点数，-1.0 = 最大左转，+1.0 = 最大右转，0.0 = 直行）
- **任务类型**：回归问题（预测连续的转向角度值）
- **模型目标**：学习从图像到转向角度的映射函数 f: Image → Steering

**数据特点**：
- **时序性**：数据按时间顺序采集，相邻帧高度相关
- **不平衡性**：直行数据（steering ≈ 0）占比较高，转弯数据较少
- **真实性**：虽然来自模拟器，但具有真实驾驶场景的基本特征
- **多样性**：包含直道、弯道、桥梁等多种场景

**数据预处理**：
1. 图像调整：从 320×160 缩放到 200×66（NVIDIA 论文标准输入）
2. 归一化：像素值从 [0, 255] 归一化到 [0, 1]
3. 转向角度：直接使用原始值，无需归一化

**为什么使用子集（1000条）？**
- 加速训练演示（CPU 环境约 2-3 分钟完成）
- 模拟真实场景中数据采集成本高的情况
- 便于观察小数据集下的过拟合现象
- 降低存储与带宽需求

**扩展实验建议**：
- 使用完整数据集（8000+ 条）观察性能提升
- 添加左右摄像头图像进行数据增强（转向角度补偿 ±0.2）
- 过滤低速数据（speed < 5 mph）以提高数据质量
- 使用随机亮度、阴影等图像增强技术

**与 CIFAR-10 对比**：

| 维度 | CIFAR-10 | Behavioral Cloning |
|------|----------|-------------------|
| 任务类型 | 分类（10类） | 回归（转向角度） |
| 输入维度 | 32×32×3 | 200×66×3 |
| 输出维度 | 10个类别概率 | 1个连续值 |
| 数据量 | 50,000（本实验5000） | 8,000+（本实验1000） |
| 评价指标 | 准确率（Accuracy） | 均方误差（MSE） |
| 应用场景 | 图像识别基础 | 端到端驾驶控制 |

**数据下载与准备**：


1) 下载公开镜像数据（脚本）：`tutorial_runs/behavioral_cloning_download.py`  
2) 准备依赖：numpy, torch, Pillow, matplotlib  
3) 确认数据目录：`tutorial_runs/behavioral_cloning_data/`  

### 4.3 训练步骤（行为克隆）

1) 训练模型（脚本）：`tutorial_runs/behavioral_cloning_train.py`  
2) 查看输出目录：`tutorial_runs/output/`  

输出文件（本次运行）：  
- `tutorial_runs/output/behavioral_cloning_model.pth`  
- `tutorial_runs/output/behavioral_cloning_train_log.csv`  
- `tutorial_runs/output/behavioral_cloning_metrics.json`  
- `tutorial_runs/output/behavioral_cloning_loss.png`  

### 4.4 评估与可视化

查看 `behavioral_cloning_loss.png`，作为“模型可用性”的直观指标。

可视化呈现（评估结果）：  
- 使用损失曲线作为评估可视化  
- 可标注 val_loss 作为对比指标  
![Behavioral Cloning Loss](tutorial_runs/output/behavioral_cloning_loss.png)
解释：
- 若 val_loss 明显高于 train_loss，说明泛化较弱
- 若曲线震荡明显，建议降低学习率或增加数据量

：  
- 训练日志包含至少 2 个 epoch  
- 权重文件成功生成  
- 损失曲线可读  

### 4.5 评估指标解读

#### 4.5.1 MSE 的含义

MSE 是预测与真实的平均平方误差。数值越小表示预测越接近。本课程只看趋势，不追求极小值。

：
- 推理输出可被加载
- 预测与真实在同一时间尺度
- 生成至少 1 张对比图

### 4.6 基于 CLIP 的语言条件相似度评分示例

目标：用 CLIP 做“语言条件的视觉相似度评分”，用于说明“语言条件偏好/评分信号”的构造方式；不等同于真实 VLA 的策略学习或动作控制。  

步骤：  
1) 安装依赖：`python -m pip install open_clip_torch`  
2) 运行脚本：`tutorial_runs/vla_clip_scoring.py`  
3) 查看输出目录：`tutorial_runs/output/`  

输出文件（生成后）：  
- `tutorial_runs/output/vla_clip_scores.json`  
- `tutorial_runs/output/vla_clip_scores.png`  

可视化呈现（评估结果）：  
- 热力图展示“图像 vs 提示词”的相似度  
- 解释：分数越高，表示该图像更符合该语言提示  
![VLA CLIP Scores](tutorial_runs/output/vla_clip_scores.png)

结果摘要（Top-1 文本提示词）：  
- center_2018_07_16_17_14_50_500.jpg -> turn left (0.2716)  
- center_2018_07_16_17_12_15_209.jpg -> turn left (0.2539)  
- center_2018_07_16_17_11_51_617.jpg -> turn left (0.2667)  
- center_2018_07_16_17_15_17_143.jpg -> turn left (0.2668)  
- center_2018_07_16_17_13_02_286.jpg -> turn left (0.2754)  
- center_2018_07_16_17_12_55_784.jpg -> turn left (0.2572)  

注意：首次运行会下载 CLIP 预训练权重，可能需要较长时间。  

---

## 实操结果记录（本次运行）

### 视觉基础实操（CNN vs ResNet）

**训练配置（已更新）**：
- 训练轮数：10 epochs（充分训练）
- 训练样本：5000张（CIFAR-10子集）
- 测试样本：1000张
- 优化器：Adam (lr=1e-3)

**结果摘要（10 epochs）**：

| 模型 | 最终训练准确率 | 最终验证/评估准确率（val） | 训练时间 | 过拟合程度 |
|------|--------------|--------------|---------|----------|
| Simple CNN | 0.9508 | 0.5730 | 21.16秒 | 0.3778 |
| ResNet18 | 0.8656 | 0.5060 | 175.37秒 | 0.3596 |

![训练过程对比](tutorial_runs/output/training_comparison.png)

**关键发现与分析**：

1. **训练能力差异显著**
   - 两个模型都能把训练集准确率拉到很高（Simple CNN 95.08%，ResNet18 86.56%）
   - 训练集指标只能说明“拟合能力”，不能直接代表“泛化能力”

2. **但验证/评估表现出现反转**
   - Simple CNN 验证/评估准确率 57.30%，高于 ResNet18 的 50.60%
   - 这是一个非常重要的教学案例：训练集更好 ≠ 测试集更好

3. **过拟合现象明显**
   - Simple CNN 过拟合程度：37.78%（训练-验证差距）
   - ResNet18 过拟合程度：35.96%（训练-验证差距）
   - **小数据子集下，两者都出现明显过拟合，需要靠泛化增强手段改善**

**工程启示**：

✅ **模型越大不等于效果越好**
- 在小数据集（5000样本）上，ResNet18 的约 1118 万参数对泛化不一定有利
- Simple CNN 约 107 万参数更“轻量”，在本次设置下验证/评估表现更好

✅ **算力成本差异巨大**
- ResNet18 训练时间约是 Simple CNN 的 8.3 倍
- 在资源受限场景（如车载芯片），这是关键考量

✅ **真实场景的权衡思维**
- 数据量、模型复杂度、算力资源需要匹配
- "能用的模型"比"最强的模型"更有工程价值

✅ **如何改进 ResNet18**
- 增加数据量（完整50000训练集）
- 添加正则化（Dropout、Weight Decay）
- 使用数据增强（随机翻转、裁剪）
- 降低学习率或使用学习率调度

**这个实验完美展示了工程实践中的核心原则：**
**"合适的模型 > 最强的模型"**

---

### 行为克隆实操（Udacity Behavioral Cloning）

数据来源（镜像）：BehavioralCloningTrackData（随机采样 1000 条）  

训练日志（MSE）：  
```
epoch 1 train_loss 0.036373 val_loss 0.045069
epoch 2 train_loss 0.028046 val_loss 0.044873
```
解释：
- train_loss 下降说明模型在收敛
- val_loss 下降幅度较小，说明泛化仍有限（小样本原因）

输出文件：
- `tutorial_runs/output/behavioral_cloning_model.pth`  
- `tutorial_runs/output/behavioral_cloning_train_log.csv`  
- `tutorial_runs/output/behavioral_cloning_metrics.json`  
- `tutorial_runs/output/behavioral_cloning_loss.png`  
![Behavioral Cloning Loss](tutorial_runs/output/behavioral_cloning_loss.png)

---

## 附录 A 开源数据与代码来源

- Udacity Behavioral Cloning 项目（本课程实操）：https://github.com/udacity/CarND-Behavioral-Cloning-P3  
- 行为克隆公开数据镜像（用于离线训练）：https://github.com/KansaiUser/BehavioralCloningTrackData  
- Udacity Simulator（可选扩展）：https://github.com/udacity/self-driving-car-sim  
- nuScenes（可选拓展，需注册）：https://www.nuscenes.org/  
- KITTI（可选拓展）：http://www.cvlibs.net/datasets/kitti/  

---

## 附录 B 目录结构与最短流程

实际目录结构（本教程）：
- `tutorial_runs/behavioral_cloning_download.py`  
- `tutorial_runs/behavioral_cloning_train.py`  
- `tutorial_runs/behavioral_cloning_data/`  
- `tutorial_runs/output/`  

最短流程：
1) `python tutorial_runs/behavioral_cloning_download.py`  
2) `python tutorial_runs/behavioral_cloning_train.py`  
3) 查看 `tutorial_runs/output/behavioral_cloning_loss.png`  

---

End of Document





