# 深度学习及智能驾驶模型实操教程（教学最小闭环版）

---

## 阅读路径

- 建议按 1 -> 2 -> 3 -> 4 -> 5 的顺序阅读与实操  
- 每章结构统一：目标 -> 知识点 -> 步骤 -> 结果 -> 检查点  
- 若只想跑通闭环，可先看第 3 章与第 5 章  

---

## 目录

1. 深度学习视觉基础与里程碑网络（Day 1 上午）
2. 深度学习基础与世界模型入门（Day 1 上午）
3. 深度学习最小实操闭环（Day 1 下午）
4. 智能驾驶相关模型概念对照（Day 2 上午）
5. 智能驾驶主题实操（Day 2 下午）

---

## 第一章 深度学习视觉基础与里程碑网络（Day 1 上午）

**目标**
- 建立视觉模型的发展脉络（CNN -> ResNet）
- 让学员知道“感知模型是如何一步步变强的”

**核心要点**
- CNN 的核心思想：局部感受野与参数共享
- 里程碑网络：LeNet、AlexNet、VGG、ResNet（概念级）
- ResNet 的关键点：残差连接缓解深层网络训练难题

**知识点详解（是什么/为什么/如何用）**
- CNN 是什么：用卷积核提取图像局部特征  
  为什么重要：视觉感知的基础结构  
  如何使用：用于分类/检测/分割等视觉任务  
- ResNet 是什么：引入残差连接的深层 CNN  
  为什么重要：更深网络更稳定、性能更好  
  如何使用：作为视觉主干网络的常用选择  

**实操案例（轻量入门）**
- 任务：在 CIFAR-10 上比较“简单 CNN vs ResNet18”  
- 结果：得到两组训练日志与准确率对比  
- 产出：日志 + 结果截图或表格  

**操作步骤（简化）**
1) 安装依赖（含 torchvision）：`python -m pip install torchvision`  
2) 运行脚本 `tutorial_runs/cifar_cnn_resnet.py`  
3) 记录两者的训练损失与准确率  

**脚本与产出**
- 脚本：`tutorial_runs/cifar_cnn_resnet.py`  
- 输出：`tutorial_runs/output/simple_cnn_results.json`  
- 输出：`tutorial_runs/output/resnet18_results.json`  
- 汇总：`tutorial_runs/output/cnn_resnet_summary.json`  

**检查点**
- 能说出 CNN 与 ResNet 的结构差异  
- 能读懂训练日志中的 loss/acc  

**完成标准**
- 至少完成一次 CNN 或 ResNet 训练  
- 记录 1 组训练结果用于对比  

---

## 第二章 深度学习基础与世界模型入门（Day 1 上午）

**目标**
- 建立“世界状态/世界模型”的最小概念闭环
- 解释世界模型在智能驾驶中的作用链条

**核心要点**
- 世界模型在智能驾驶中的作用链条：感知表示 -> 状态 -> 预测 -> 规划/控制
- 实操目标定义：输入 = 预处理后的时序特征/状态，输出 = 下一时刻状态预测
- 从经典机器学习到深度学习的关键变化
- 本课程中的“世界状态”示例（概念级）：
  - 形式 A：时序特征向量（T x D）
  - 形式 B：时序栅格/BEV 表示（T x H x W x C）
  - 课程实操使用哪一种，以实际代码与数据为准

**知识点详解（是什么/为什么/如何用）**
- 世界状态是什么：对场景的统一数值表示，不是检测框集合  
  为什么重要：统一表示才能被预测与复用  
  如何使用：作为模型输入，预测下一时刻状态  
- 世界模型是什么：学习状态在时间上的变化规律  
  为什么重要：让系统具备“可预测性”  
  如何使用：用前 T 帧状态预测下一帧  
- 输入/输出是什么：输入为时序状态，输出为下一时刻状态  
  为什么重要：明确数据流，便于训练与评估  
  如何使用：对齐预处理输出与模型输入格式  

**学习步骤（讲解节奏）**
1) 用 3 分钟讲清“世界状态是什么，不是什么”  
2) 解释“世界模型=预测状态演化”，并给出输入/输出对照  
3) 把链条串起来：感知表示 -> 状态 -> 预测 -> 规划/控制  
4) 说明本课程只完成“状态 -> 预测”的最小闭环  

**新手理解提示**
- 世界状态不是检测框或轨迹本身，而是“对场景的统一表示”  
- 世界模型不是控制器，而是“对状态的时间预测器”  
- 本课程只做“预测下一步”，不做闭环控制  

**检查点**
- 学员能用 2 句话解释世界状态与世界模型  
- 学员能说清本课程实操的输入/输出  

**常见疑问**
- Q: 世界状态是不是检测框/轨迹？  
  A: 不是，它是“可被后续模块消费的统一表示”。  
- Q: 我们做的是完整自动驾驶吗？  
  A: 不是，仅完成最小学习闭环。  

**练习产出**
- 1 页概念小结（输入/输出/作用链条）

**完成标准**
- 能写出“输入是什么、输出是什么、为什么做预测”  
- 能把“感知 -> 状态 -> 预测 -> 控制”说成一句话  

---

## 第三章 深度学习最小实操闭环（Day 1 下午）

**目标**
- 完成环境、数据、训练的最小闭环

**环境与资源要求（本机训练）**
- 操作系统：Windows 10/11  
- Python：3.10.x  
- 硬件：CPU 可运行；推荐 RAM >= 8GB  
- 依赖：numpy, matplotlib, torch（CPU 版即可）  
- 预计时长：数据加载 < 5 分钟，训练 < 5 分钟  

**实操内容**
- 环境与依赖准备（完成运行验证）
- 数据准备与预处理（生成可训练样例数据集）
- 训练最简世界模型（生成权重与训练日志）

**操作步骤**
1) 环境检查：确保 Python 与依赖可用  
2) 数据准备：下载/整理样例数据，确认目录结构  
3) 预处理：生成可训练的时序状态数据  
4) 训练：启动训练脚本，观察损失变化  
5) 保存：确认权重与日志输出成功  

**结果应是什么样**
- 预处理阶段：生成新的可训练数据文件或缓存  
- 训练阶段：出现持续输出的训练日志  
- 结束阶段：模型权重文件可被再次加载  

**训练流程解释（是什么/为什么/如何用）**
- 数据：提供连续帧，让模型学习时间规律  
  为什么重要：没有时序就无法学“预测”  
  如何使用：按 T 帧输入、1 帧输出切分  
- 模型：简单 CNN，以时间维作为通道  
  为什么重要：结构简单，便于新手跑通  
  如何使用：输入张量形状与通道数匹配  
- 损失：MSE 评估预测与真实差异  
  为什么重要：提供可优化目标  
  如何使用：训练日志中观察下降趋势  

**检查点**
- 能成功导入核心依赖并运行最小验证脚本  
- 预处理后产生可训练的数据文件  
- 训练日志中出现损失下降或稳定输出  
- 权重文件在指定目录生成  

**常见问题**
- 依赖装不全 -> 检查 Python 版本与包冲突  
- 数据路径错误 -> 核对数据目录与配置  
- 训练太慢 -> 降低 batch 或使用更小数据子集  

**练习产出**
- 训练日志与模型权重文件

**完成标准**
- 至少 1 次训练完整跑完  
- 权重文件可用于后续推理  

---

## 第四章 智能驾驶相关模型概念对照（Day 2 上午）

**目标**
- 把智能驾驶前沿概念与本次实操闭环对齐

**核心要点（作用级）**
- 感知融合：对应数据对齐与特征融合的概念位置
- BEV 表示：对应空间表示的理解方式
- 3D 重建：对应几何直观理解
- 世界模型时序建模：对应下一时刻预测
- VLM/VLA 语义约束：对应“状态语义化”概念
- 状态驱动控制：对应“预测结果如何被使用”
- 模型工程化流程：训练 -> 推理 -> 导出的最小链路

**概念对照（快速记忆版）**
- 感知融合 = 多传感器信息的对齐思想  
- BEV/3D = 空间结构的统一视角  
- 世界模型 = 时间维度上的预测器  
- VLM/VLA = 用语言约束或解释状态  
- 控制 = 预测结果的消费方  
- 工程化 = 训练到推理的流程可复现  

**知识点详解（是什么/为什么/如何用）**
- 感知融合是什么：把多源信息对齐到同一坐标  
  为什么重要：统一表达才能下游消费  
  如何用在本课程：理解预处理与特征对齐的意义  
- BEV/3D 是什么：从俯视/三维方式理解场景结构  
  为什么重要：便于几何直观理解  
  如何用在本课程：用于解释可视化结果  
- VLM/VLA 是什么：用语言为状态提供语义约束  
  为什么重要：便于解释和对齐高层目标  
  如何用在本课程：仅做概念理解，不做实现  

**学习步骤（讲解节奏）**
1) 先讲“本课程做了什么”，再讲“行业怎么做更复杂”  
2) 每个概念只讲 1 句话作用与位置  
3) 用“概念 -> 实操环节”做对应  

**检查点**
- 学员能把“一个概念 -> 一个实操环节”说清楚  

**练习产出**
- 1 页概念对照表（概念 -> 对应实操环节）

**完成标准**
- 学员能口头复述 3 个概念与实操的对应关系  

---

## 第五章 智能驾驶主题实操（Day 2 下午）

**目标**
- 完成推理与可视化的实操闭环

**实操内容**
- 加载权重并运行推理，生成下一时刻状态预测
- 结果对齐与一致性检查（对照输入与预测）
- 生成真实 vs 预测对比图（使用脚本输出或手动打开图片）
- 可选：保存推理结果文件或导出模型

**操作步骤**
1) 加载训练权重并运行推理（可复用训练脚本的评估步骤）  
2) 检查预测结果格式与输入一致  
3) 进行一致性检查（时间对齐、形状一致、数值范围合理）  
4) 查看生成的对比图（prediction_compare.png）  
5) 保存结果文件，便于后续复盘  

**结果应是什么样**
- 预测结果与真实结果可在同一画面比较  
- 预测趋势与真实趋势大致一致（不要求精度最优）  

**评估指标解读（是什么/为什么/如何用）**
- MSE 是什么：预测与真实的平均平方误差  
  为什么重要：数值越小表示预测越接近  
  如何用在本课程：只看趋势，不追求极小值  

**检查点**
- 推理输出文件可被加载  
- 预测结果与真实结果在同一坐标/时间尺度  
- 至少生成 1 张可视化对比图  

**常见问题**
- 推理报错 -> 检查权重路径与模型版本  
- 结果错位 -> 检查时间步或序列索引  
- 可视化为空 -> 检查输出文件路径与格式  

**练习产出**
- 可加载的预测结果文件
- 至少 1 张可视化对比图

**完成标准**
- 推理与可视化全流程跑通  
- 产出物可复用（可再次加载与展示）

---

## 实操案例与结果记录（本次运行）

### 视觉基础实操结果（CNN vs ResNet）
- 数据集：CIFAR-10（torchvision 自动下载）  
- Python：3.10.8  
- PyTorch：2.9.1+cpu  
- torchvision：0.24.1+cpu  
- 训练设置：子集 5000/1000，单 epoch  
- 输出文件：  
  - `tutorial_runs/output/simple_cnn_results.json`  
  - `tutorial_runs/output/resnet18_results.json`  
  - `tutorial_runs/output/cnn_resnet_summary.json`  

**结果摘要**
```
simple_cnn: train_acc 0.2792, test_acc 0.3740
resnet18:  train_acc 0.3388, test_acc 0.3210
```

---

### 数据与代码下载
- 开源代码与数据仓库：https://github.com/tychovdo/MovingMNIST  
- 版本（commit）：62728f566255a15d512b341aa4ee29977aaa7497  
- 数据文件：mnist_test_seq.npy.gz（仓库内置）  
- 下载命令：`git clone https://github.com/tychovdo/MovingMNIST`  
- 本地路径：`third_party/MovingMNIST`  

### 训练配置
- Python：3.10.8  
- PyTorch：2.9.1+cpu  
- 输入序列长度：10 帧  
- 预测目标：下一时刻 1 帧  
- 训练集/测试集：1000 / 200  
- 训练轮数：2  
- Batch size：32  
- 学习率：1e-3  
- 模型：简单 CNN（以时间维作为通道输入）  
- 评估指标：MSE  

### 训练与评估结果
```
epoch 1 train_loss 0.032198
epoch 2 train_loss 0.025859
test_loss 0.024536
```

### 产出文件
- 训练脚本：`tutorial_runs/moving_mnist_train.py`  
- 训练日志：`tutorial_runs/output/train_log.txt`  
- 模型权重：`tutorial_runs/output/moving_mnist_simple_world_model.pth`  
- 评估指标：`tutorial_runs/output/metrics.json`  
- 可视化图：`tutorial_runs/output/prediction_compare.png`  

---

## 附录 A 开源数据与代码来源说明（教学简化版）
（与课程内容一致，便于学员自查来源与使用范围）

本说明面向基础教学，目的是让学员了解：数据与代码来源、使用范围与合规边界，仅用于教学/研究/PoC 级验证。

### 学员只需记住的 3 件事
1) 全部基于公开开源数据与开源代码  
2) 只用于教学 / 研究 / PoC 级验证  
3) 不用于商业量产或正式产品发布  

### 主要数据来源（教学用）
- MovingMNIST（本课程实操数据，开源仓库内置）  
  - https://github.com/tychovdo/MovingMNIST  
- nuScenes Dataset（可选拓展，需注册）  
  - https://www.nuscenes.org/  
- KITTI Dataset（可选拓展）  
  - http://www.cvlibs.net/datasets/kitti/  

### 主要开源框架与工具
- PyTorch / TorchVision  
- HuggingFace Transformers  
- CLIP  
- ONNX / ONNX Runtime  
- JupyterLab  

### 教学使用提示
- MovingMNIST 为仓库内置数据，无需额外下载子集  
- nuScenes/KITTI 仅作为可选拓展  
- 只完成最小学习闭环，不做工程化训练或量产验证  

### 合规简明声明（讲师可直接引用）
本课程及配套代码完全基于公开开源数据与开源代码，仅用于教学与研究验证，不包含任何商业闭源组件或私有算法，不涉及商业量产或产品发布。

---

## 附录 B 实操代码项目结构说明（教学最小闭环版）
（与课程实操一致，帮助学员理解目录结构与运行路径）

### 文档定位
面向“基础学员 + 讲师经验不足”，目标是完成最小学习闭环：开源数据 -> 训练世界模型 -> 推理预测 -> 可视化输出。

### 学员最终产出（最小标准）
1) 成功跑通训练脚本并生成模型权重  
2) 完成一次推理，输出未来状态预测结果  
3) 生成至少 1 张对比可视化图（真实 vs 预测）  
4) 提交 1 页简短学习记录（问题/方法/结果/反思）  

### 最小闭环路线图（唯一必学路径）
third_party/MovingMNIST/ -> tutorial_runs/moving_mnist_train.py -> tutorial_runs/output/

- third_party/MovingMNIST/: 数据与示例代码  
- tutorial_runs/moving_mnist_train.py: 训练 + 推理 + 可视化  
- tutorial_runs/output/: 日志、权重与对比图  

### 本教程实际目录结构（本次实操）
third_party/MovingMNIST/  
tutorial_runs/moving_mnist_train.py  
tutorial_runs/output/  

### 本教程最短运行流程（本次实操）
1) `git clone https://github.com/tychovdo/MovingMNIST`  
2) 安装 torch（CPU）：`python -m pip install torch --index-url https://download.pytorch.org/whl/cpu`  
3) 运行 `tutorial_runs/moving_mnist_train.py`  
4) 查看 `tutorial_runs/output/` 的日志、权重与可视化  

### 行业参考路线图（扩展阅读）
data/ -> world_model/ -> notebooks/

- data/: 使用 nuScenes mini/sample 数据完成预处理  
- world_model/: 训练最简世界模型并预测下一时刻状态  
- notebooks/: 可视化对比结果，完成学习总结  

> 其他模块（perception / multimodal / control / engineering）为扩展阅读，可在后续进阶使用。

### 跑通标准（完成定义）
- 训练能够正常结束  
- 推理输出预测序列  
- 至少生成 1 张可视化结果图  

### 常见问题与排查（面向新手）
1) 数据路径错误 -> 检查 data/ 目录配置  
2) 依赖缺失 -> 重新安装依赖列表  
3) GPU/CPU 不匹配 -> 降低 batch size 或切换 CPU  
4) 张量形状报错 -> 检查预处理输出维度  
5) 内存不足 -> 缩小 sample 或减少并行  
6) 模型权重找不到 -> 检查训练输出路径  

---

End of Document
